{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single latent factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hybrid(Model):\n",
    "    def __init__(self, alpha, n_factors, lambda_reg=0):\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.alpha = alpha\n",
    "        self.n_factors = n_factors\n",
    "        \n",
    "    def predict_season_scores(self, season_scores):\n",
    "        predicted_score_table = pd.DataFrame(self.skater_scores.values @ self.event_scores.values.T + self.baseline,\n",
    "                                            index=self.skater_scores.index,\n",
    "                                            columns=self.event_scores.index)\n",
    "        \n",
    "        predicted_score_stacked = predicted_score_table.stack()\n",
    "        season_skater_event_index = season_scores.set_index(['name', 'event']).index\n",
    "        return predicted_score_stacked.loc[season_skater_event_index].values\n",
    "        \n",
    "    def fit(self, season_scores, n_iter, seed=42, verbose=False):\n",
    "        season_pivot = pd.pivot_table(season_scores[['name', 'event', 'score']], values='score', index='name', columns='event')\n",
    "        skater_names = list(season_pivot.index)\n",
    "        event_names = list(season_pivot.columns)\n",
    "        true_scores = season_pivot.values\n",
    "        \n",
    "        random_state = np.random.RandomState(seed=seed)\n",
    "        self.skater_scores = random_state.random_sample((len(skater_names), self.n_factors))\n",
    "        self.event_scores = random_state.random_sample((self.n_factors, len(event_names)))\n",
    "        self.baseline = random_state.random_sample()\n",
    "        \n",
    "        for i in range(n_iter):\n",
    "            predicted_scores = self.skater_scores @ self.event_scores + self.baseline\n",
    "            diff = predicted_scores - true_scores\n",
    "            \n",
    "            for k in range(self.n_factors):\n",
    "                skater_score_k = self.skater_scores[:, [k]]\n",
    "                event_score_k = self.event_scores[[k], :]\n",
    "                \n",
    "                baseline_gradient = np.nansum(diff)                \n",
    "                skater_gradients = np.nansum(diff * event_score_k, axis=1, keepdims=True) + self.lambda_reg * skater_score_k\n",
    "                event_gradients = np.nansum(diff * skater_score_k, axis=0, keepdims=True) + self.lambda_reg * event_score_k \n",
    "                \n",
    "                self.baseline = self.baseline - self.alpha * baseline_gradient\n",
    "                self.skater_scores[:, [k]] = skater_score_k - self.alpha * skater_gradients\n",
    "                self.event_scores[[k], :] = event_score_k - self.alpha * event_gradients\n",
    "                \n",
    "            # Print difference in RMSE for last two iterations\n",
    "            if i == (n_iter-1):\n",
    "                rmse_old = np.sqrt(np.nanmean(diff**2))\n",
    "                diff = self.skater_scores @ self.event_scores + self.baseline - true_scores\n",
    "                rmse_new = np.sqrt(np.nanmean(diff**2))\n",
    "                print(f'Alpha: {self.alpha}, Lambda: {self.lambda_reg}, Iter: {n_iter}, Last RMSE: {round(rmse_new, 2)}, Delta RMSE: {round(rmse_new - rmse_old, 10)}')\n",
    "\n",
    "        self.skater_scores = pd.DataFrame(self.skater_scores, index=skater_names)\n",
    "        self.event_scores = pd.DataFrame(self.event_scores.T, index=event_names)\n",
    "        \n",
    "        self.skater_scores.sort_values(by=0, ascending=False, inplace=True)\n",
    "        self.event_scores.sort_values(by=0, ascending=False, inplace=True)\n",
    "        \n",
    "\n",
    "    def evaluate_rmse_over_years(self, years, season_df, world_df, **kwargs):\n",
    "        rmses = []\n",
    "        for year in years:\n",
    "            season_scores, world_scores = get_yearly_scores(year, season_df, world_df)\n",
    "            self.fit(season_scores, **kwargs)\n",
    "            rmse = self.evaluate_rmse(season_scores)\n",
    "            rmses.append(rmse)\n",
    "        return pd.DataFrame({'year': years, 'rmse': rmses}).sort_values(by='year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train over all years in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid = Hybrid(alpha=0.001, n_factors=1, lambda_reg=10)\n",
    "hybrid_train_eval = hybrid.evaluate_over_years(train_years, season_train, world_train, \n",
    "                    n_iter=1000, seed=42)\n",
    "hybrid_train_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for lambda_reg in [0, 0.1, 1, 2, 5, 10]:\n",
    "    hybrid = Hybrid(lambda_reg=lambda_reg, alpha=0.001, n_factors=1)\n",
    "    hybrid_train_eval = hybrid.evaluate_over_years(train_years, season_train, world_train, \n",
    "                                                   n_iter=1000, seed=42)\n",
    "    print(lambda_reg, hybrid_train_eval['rmse'].mean(), hybrid_train_eval['tau'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid = Hybrid(alpha=0.001, n_factors=1, lambda_reg=1)\n",
    "hybrid.fit(season_scores, n_iter=1000, seed=42)\n",
    "hybrid_train_eval = hybrid.evaluate_over_years(train_years, season_train, world_train, \n",
    "                    n_iter=1000, seed=42)\n",
    "hybrid_train_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lambda_reg in [0, 1]:\n",
    "    hybrid = Hybrid(lambda_reg=lambda_reg, alpha=0.001, n_factors=1)\n",
    "    hybrid_test_eval = hybrid.evaluate_over_years(test_years, season_test, world_test,\n",
    "                                                  n_iter=1000, seed=42)\n",
    "    print(lambda_reg, hybrid_test_eval['rmse'].mean(), hybrid_test_eval['tau'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = AverageScore()\n",
    "avg_test_eval = avg.evaluate_over_years(test_years, season_test, world_test)\n",
    "avg_test_eval['tau'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple latent factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(seed=42)\n",
    "f1_years = list(random_state.choice(train_years, 5, replace=False))\n",
    "f2_years = [year for year in train_years if year not in f1_years]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season_and_world_scores(scores):\n",
    "    all_season_scores = {}\n",
    "    all_world_scores = {}\n",
    "\n",
    "    for year in range(2005, 2020):\n",
    "        season_scores = scores.loc[(scores['year']==year) & (scores['event']!='WR')].copy()\n",
    "        world_scores = scores.loc[(scores['year']==year) & (scores['event']=='WR'), ['name', 'score']]\n",
    "        world_scores = world_scores.set_index('name').squeeze()\n",
    "        all_season_scores[year] = season_scores\n",
    "        all_world_scores[year] = world_scores\n",
    "    return all_season_scores, all_world_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_season_scores, all_world_scores = get_season_and_world_scores(male_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_normalized_scores = {}\n",
    "all_pair_diffs = {}\n",
    "\n",
    "for year in train_years:\n",
    "    season_scores = all_season_scores[year]\n",
    "    world_scores = all_world_scores[year]\n",
    "    hybrid = Hybrid(alpha=0.001, n_factors=5, lambda_reg=10)\n",
    "    hybrid.fit(season_scores, n_iter=1000, seed=42)    \n",
    "    hybrid_scores = hybrid.skater_scores\n",
    "    \n",
    "    normalized_scores = (hybrid_scores - hybrid_scores.mean(axis=0)) / hybrid_scores.std(axis=0)\n",
    "    normalized_scores = normalized_scores.reindex(world_scores.index).dropna()\n",
    "    all_normalized_scores[year] = normalized_scores\n",
    "    \n",
    "    pair_diffs = np.array(list(skater1 - skater2 for skater1, skater2 in combinations(normalized_scores.values, 2)))\n",
    "    all_pair_diffs[year] = pair_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.vstack((all_pair_diffs[year] for year in f2_years))\n",
    "y_train = np.full(len(X_train), 1)\n",
    "n_coefs = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = BatchLogistic(theta=np.full(n_coefs, 0.5), alpha=0.001, lambda_reg=10)\n",
    "log.fit(X_train, y_train, n_iter=1000)\n",
    "log.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_taus = []\n",
    "for year in f1_years:\n",
    "    print(year)\n",
    "    normalized_scores = all_normalized_scores[year]\n",
    "    combined_scores =  pd.Series(normalized_scores @ log.theta, index=normalized_scores.index)\n",
    "    combined_ranking, world_ranking = return_ranking(combined_scores, all_world_scores[year])\n",
    "    f1_taus.append(calculate_kendall_tau(combined_ranking, world_ranking))\n",
    "    \n",
    "print(np.array(f1_taus).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2_taus = []\n",
    "for year in f2_years:\n",
    "    print(year)\n",
    "    normalized_scores = all_normalized_scores[year]\n",
    "    combined_scores =  pd.Series(normalized_scores @ log.theta, index=normalized_scores.index)\n",
    "    combined_ranking, world_ranking = return_ranking(combined_scores, all_world_scores[year])\n",
    "    f2_taus.append(calculate_kendall_tau(combined_ranking, world_ranking))\n",
    "    \n",
    "print(np.array(f2_taus).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridLog:\n",
    "    def __init__(self, n_factors, hybrid_lambda,\n",
    "              hybrid_alpha=0.001, hybrid_iter=1000, hybrid_seed=42,\n",
    "              log_alpha=0.001, log_iter=1000):\n",
    "        self.n_factors = n_factors\n",
    "        self.hybrid_lambda = hybrid_lambda\n",
    "        self.hybrid_alpha = hybrid_alpha\n",
    "        self.hybrid_iter = hybrid_iter\n",
    "        self.hybrid_seed = hybrid_seed\n",
    "        self.log_alpha = log_alpha\n",
    "        self.log_iter = log_iter\n",
    "\n",
    "    \n",
    "    def fit_hybrid(self, season_df, world_df, train_years):\n",
    "        # Train hybrid model on each training years to get latent factor values\n",
    "        all_pair_diffs = {}\n",
    "        for year in train_years:\n",
    "            season_scores, world_scores = get_yearly_scores(year, season_df, world_df)\n",
    "            hybrid = Hybrid(alpha=self.hybrid_alpha, n_factors=self.n_factors, lambda_reg=self.hybrid_lambda)\n",
    "            hybrid.fit(season_scores, n_iter=self.hybrid_iter, seed=self.hybrid_seed)\n",
    "            hybrid_skater_scores = hybrid.skater_scores\n",
    "\n",
    "            normalized_scores = (hybrid_skater_scores - hybrid_skater_scores.mean(axis=0)) / hybrid_skater_scores.std(axis=0)\n",
    "            normalized_scores = normalized_scores.reindex(world_scores.index).dropna()\n",
    "\n",
    "            pair_diffs = np.array(list(skater1 - skater2 for skater1, skater2 in combinations(normalized_scores.values, 2)))\n",
    "            all_pair_diffs[year] = pair_diffs\n",
    "\n",
    "        # Train logistic regression on pairwise differences of latent factor values\n",
    "        self.X_train = np.vstack((all_pair_diffs[year] for year in train_years))\n",
    "        self.y_train = np.full(len(self.X_train), 1)\n",
    "    \n",
    "    \n",
    "    def fit_log(self, log_lambda):\n",
    "        log = BatchLogistic(theta=np.full(self.n_factors, 0.5), alpha=self.log_alpha, \n",
    "                            lambda_reg=log_lambda)\n",
    "        log.fit(self.X_train, self.y_train, n_iter=self.log_iter)\n",
    "        self.log_coefs = log.theta\n",
    "\n",
    "    \n",
    "    def predict(self, season_scores):\n",
    "        hybrid = Hybrid(alpha=self.hybrid_alpha, n_factors=self.n_factors, lambda_reg=self.hybrid_lambda)\n",
    "        hybrid.fit(season_scores, n_iter=self.hybrid_iter, seed=self.hybrid_seed)    \n",
    "        hybrid_skater_scores = hybrid.skater_scores\n",
    "\n",
    "        normalized_scores = (hybrid_skater_scores - hybrid_skater_scores.mean(axis=0)) / hybrid_skater_scores.std(axis=0)    \n",
    "        combined_scores = pd.Series(normalized_scores @ self.log_coefs, index=normalized_scores.index)\n",
    "        combined_scores.sort_values(ascending=False, inplace=True)\n",
    "        return combined_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_kendall_tau(hybridlog, years, season_df, world_df):\n",
    "    kendall_taus = []\n",
    "    for year in years:\n",
    "        season_scores, world_scores = get_yearly_scores(year, season_df, world_df)\n",
    "        combined_scores = hybridlog.predict(season_scores)\n",
    "        combined_ranking, world_ranking = return_ranking(combined_scores, world_scores)\n",
    "        kendall_tau = calculate_kendall_tau(combined_ranking, world_ranking, verbose=False)\n",
    "        kendall_taus.append(kendall_tau)\n",
    "\n",
    "    return np.array(kendall_taus).mean()\n",
    "\n",
    "\n",
    "def get_tau_train_val(season_df, world_df, train_years, val_years,\n",
    "                      hybridlog, log_lambda):\n",
    "    hybridlog.fit_log(log_lambda)\n",
    "    avg_tau_train = average_kendall_tau(hybridlog, train_years, season_df, world_df)\n",
    "    avg_tau_val = average_kendall_tau(hybridlog, val_years, season_df, world_df)\n",
    "    return avg_tau_train, avg_tau_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_factors = []\n",
    "hybrid_lambdas = []\n",
    "log_lambdas = []\n",
    "f1_tau_trains = []\n",
    "f1_tau_vals = []\n",
    "f2_tau_trains = []\n",
    "f2_tau_vals = []\n",
    "\n",
    "n_iter = 1000\n",
    "for n_factor in [1, ]:\n",
    "    for hybrid_lambda in [10]:\n",
    "        n_factors.append(n_factor)\n",
    "        hybrid_lambdas.append(hybrid_lambda)\n",
    "\n",
    "        # Train hybrid models on each fold\n",
    "        hybridlog1 = HybridLog(n_factors=n_factor, hybrid_lambda=hybrid_lambda,\n",
    "                            hybrid_alpha=0.001, hybrid_iter=n_iter, hybrid_seed=42,\n",
    "                            log_alpha=0.001, log_iter=n_iter)\n",
    "        hybridlog1.fit_hybrid(season_male, world_male, f1_years)\n",
    "\n",
    "        hybridlog2 = HybridLog(n_factors=n_factor, hybrid_lambda=hybrid_lambda,\n",
    "                            hybrid_alpha=0.001, hybrid_iter=n_iter, hybrid_seed=42,\n",
    "                            log_alpha=0.001, log_iter=n_iter)\n",
    "        hybridlog2.fit_hybrid(season_male, world_male, f2_years)\n",
    "\n",
    "      # Train log models on each fold and evaluate kendall tau\n",
    "    for log_lambda in [10]:\n",
    "        print(f'n_factor: {n_factor}, hybrid_lambda: {hybrid_lambda}, log_lambda: {log_lambda}')\n",
    "        log_lambdas.append(log_lambda)\n",
    "        f1_tau_train, f1_tau_val = get_tau_train_val(season_male, world_male,\n",
    "                                                     f1_years, f2_years,\n",
    "                                                     hybridlog1, log_lambda)\n",
    "        f2_tau_train, f2_tau_val = get_tau_train_val(season_male, world_male,\n",
    "                                                     f2_years, f1_years,\n",
    "                                                     hybridlog2, log_lambda)\n",
    "        print(f'f1_train: {f1_tau_train}, f1_val: {f1_tau_val}, f2_train: {f2_tau_train}, f2_val: {f2_tau_val}')\n",
    "        f1_tau_trains.append(f1_tau_train)\n",
    "        f1_tau_vals.append(f1_tau_val)\n",
    "        f2_tau_trains.append(f2_tau_train)\n",
    "        f2_tau_vals.append(f2_tau_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = pd.DataFrame({'n_factor': n_factors, 'hybrid_lambda': hybrid_lambdas, 'log_lambda': log_lambdas,\n",
    "'f1_tau_train': f1_tau_trains, 'f1_tau_val': f1_tau_vals,\n",
    "'f2_tau_train': f2_tau_trains, 'f2_tau_val': f2_tau_vals})\n",
    "comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_train_eval.loc[hybrid_train_eval['year'].isin(f2_years), 'tau'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
